{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading speech datasets\n",
      "Looking for data ./spoken_numbers_spectros_64x64.tar in data/\n",
      "Downloading from http://pannous.net/files/./spoken_numbers_spectros_64x64.tar to data/spoken_numbers_spectros_64x64.tar\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[1;32m-> 1318\u001b[1;33m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[1;32m--> 936\u001b[1;33m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[0;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m             \u001b[1;31m# Break explicitly a reference cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b383094cf52c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"downloading speech datasets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m         \u001b[0mmaybe_download\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mSource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDIGIT_SPECTROS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m         \u001b[0mmaybe_download\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mSource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDIGIT_WAVES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0mmaybe_download\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mSource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUMBER_IMAGES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b383094cf52c>\u001b[0m in \u001b[0;36mmaybe_download\u001b[1;34m(file, work_directory)\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl_filename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downloading from %s to %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murl_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprogresshook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m                 \u001b[0mstatinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Successfully downloaded'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bytes.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 544\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hetshah\\anaconda2\\envs\\tf18\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "#!/usr/local/bin/python##Utilities for downloading and providing data from openslr.org, libriSpeech, Pannous, Gutenberg, WMT, tokenizing, vocabularies.\"\"\"\n",
    "# TODO! see https://github.com/pannous/caffe-speech-recognition for some data sources\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import wave\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import skimage.io  # scikit-image\n",
    "\n",
    "try:\n",
    "\timport librosa\n",
    "except:\n",
    "\tprint(\"pip install librosa ; if you want mfcc_batch_generator\")\n",
    "# import extensions as xx\n",
    "from random import shuffle\n",
    "try:\n",
    "\tfrom six.moves import urllib\n",
    "\tfrom six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "except:\n",
    "\tpass # fuck 2to3\n",
    "\n",
    "speech_commands=\"sheila seven right one house down zero go yes wow six no three happy \\\n",
    "bird stop marvin two five on off four dog up tree cat bed nine eight left\".split(\" \")\n",
    "\n",
    "# TRAIN_INDEX='train_words_index.txt'\n",
    "# TEST_INDEX='test_words_index.txt'\n",
    "SOURCE_URL = 'http://pannous.net/files/' #spoken_numbers.tar'\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "pcm_path = \"data/spoken_numbers_pcm/\" # 8 bit\n",
    "wav_path = \"data/spoken_numbers_wav/\" # 16 bit s16le\n",
    "path = pcm_path\n",
    "CHUNK = 4096\n",
    "test_fraction=0.1 # 10% of data for test / verification\n",
    "\n",
    "class Source:  # labels\n",
    "\tDIGIT_WAVES = './spoken_numbers_pcm.tar'\n",
    "\t# DIGIT_SPECTROS = 'spoken_numbers_spectros_64x64.tar'  # 64x64  baby data set, works astonishingly well\n",
    "\tDIGIT_SPECTROS = './spoken_numbers_spectros_64x64.tar'\n",
    "\tNUMBER_WAVES = 'spoken_numbers_wav.tar'\n",
    "\tNUMBER_IMAGES = 'spoken_numbers.tar'  # width=256 height=256\n",
    "\tWORD_SPECTROS = 'https://dl.dropboxusercontent.com/u/23615316/spoken_words.tar'  # width,height=512# todo: sliding window!\n",
    "\tSPEECH_COMMANDS = \"http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\" # 16000Hz 1ch s16le (2 bytes per sample)\n",
    "\tWORD_WAVES = 'spoken_words_wav.tar'\n",
    "\tTEST_INDEX = 'test_index.txt'\n",
    "\tTRAIN_INDEX = 'train_index.txt'\n",
    "\n",
    "from enum import Enum\n",
    "class Target(Enum):  # labels\n",
    "\tdigits=1\n",
    "\tspeaker=2\n",
    "\twords_per_minute=3\n",
    "\tword_phonemes=4\n",
    "\tword = 5  # int vector as opposed to binary hotword\n",
    "\tsentence=6\n",
    "\tsentiment=7\n",
    "\tfirst_letter=8\n",
    "\thotword = 9\n",
    "\tspeech_commands = 10\n",
    "\t# test_word=9 # use 5 even for speaker etc\n",
    "\n",
    "\n",
    "num_characters = 32\n",
    "# num_characters=60 #  only one case, Including numbers\n",
    "# num_characters=128 #\n",
    "# num_characters=256 #  including special characters\n",
    "# offset=0  # 1:1 mapping ++\n",
    "# offset=32 # starting with ' ' space\n",
    "# offset=48 # starting with  numbers\n",
    "offset = 64  # starting with characters\n",
    "max_word_length = 20\n",
    "terminal_symbol = 0\n",
    "\n",
    "def pad(vec, pad_to=max_word_length, one_hot=False,paddy=terminal_symbol):\n",
    "\tfor i in range(0, pad_to - len(vec)):\n",
    "\t\tif one_hot:\n",
    "\t\t\tvec.append([paddy] * num_characters)\n",
    "\t\telse:\n",
    "\t\t\tvec.append(paddy)\n",
    "\treturn vec\n",
    "\n",
    "def char_to_class(c):\n",
    "\treturn (ord(c) - offset) % num_characters\n",
    "\n",
    "def string_to_int_word(word, pad_to):\n",
    "\tz = map(char_to_class, word)\n",
    "\tz = list(z)\n",
    "\tz = pad(z)\n",
    "\treturn z\n",
    "\n",
    "class SparseLabels:\n",
    "\tdef __init__(labels):\n",
    "\t\tlabels.indices = {}\n",
    "\t\tlabels.values = []\n",
    "\n",
    "\tdef shape(self):\n",
    "\t\treturn (len(self.indices),len(self.values))\n",
    "\n",
    "# labels: An `int32` `SparseTensor`.\n",
    "# labels.indices[i, :] == [b, t] means `labels.values[i]` stores the id for (batch b, time t).\n",
    "# labels.values[i]` must take on values in `[0, num_labels)`.\n",
    "def sparse_labels(vec):\n",
    "\tlabels = SparseLabels()\n",
    "\tb=0\n",
    "\tfor lab in vec:\n",
    "\t\tt=0\n",
    "\t\tfor c in lab:\n",
    "\t\t\tlabels.indices[b, t] = len(labels.values)\n",
    "\t\t\tlabels.values.append(char_to_class(c))\n",
    "\t\t\t# labels.values[i] = char_to_class(c)\n",
    "\t\t\tt += 1\n",
    "\t\tb += 1\n",
    "\treturn labels\n",
    "\n",
    "\n",
    "\n",
    "def progresshook(blocknum, blocksize, totalsize):\n",
    "\t\treadsofar = blocknum * blocksize\n",
    "\t\tif totalsize > 0:\n",
    "\t\t\t\tpercent = readsofar * 1e2 / totalsize\n",
    "\t\t\t\ts = \"\\r%5.1f%% %*d / %d\" % (\n",
    "\t\t\t\t\t\tpercent, len(str(totalsize)), readsofar, totalsize)\n",
    "\t\t\t\tsys.stderr.write(s)\n",
    "\t\t\t\tif readsofar >= totalsize: # near the end\n",
    "\t\t\t\t\t\tsys.stderr.write(\"\\n\")\n",
    "\t\telse: # total size is unknown\n",
    "\t\t\t\tsys.stderr.write(\"read %d\\n\" % (readsofar,))\n",
    "\n",
    "def maybe_download(file, work_directory=DATA_DIR):\n",
    "\t\"\"\"Download the data from Pannous's website, unless it's already here.\"\"\"\n",
    "\tprint(\"Looking for data %s in %s\"%(file,work_directory))\n",
    "\tif not os.path.exists(work_directory):\n",
    "\t\ttry:\n",
    "\t\t\tos.mkdir(work_directory)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\tfilepath = os.path.join(work_directory, re.sub('.*\\/','',file))\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tif not file.startswith(\"http\"): url_filename = SOURCE_URL + file\n",
    "\t\telse: url_filename=file\n",
    "\t\tprint('Downloading from %s to %s' % (url_filename, filepath))\n",
    "\t\tfilepath, _ = urllib.request.urlretrieve(url_filename, filepath,progresshook)\n",
    "\t\tstatinfo = os.stat(filepath)\n",
    "\t\tprint('Successfully downloaded', file, statinfo.st_size, 'bytes.')\n",
    "\t\t# os.system('ln -s '+work_directory)\n",
    "\tif os.path.exists(filepath):\n",
    "\t\tprint('Extracting %s to %s' % ( filepath, work_directory))\n",
    "\t\tos.system('tar xf '+filepath+\" -C \"+work_directory)\n",
    "\t\tprint('Data ready!')\n",
    "\treturn filepath.replace(\".tar\",\"\")\n",
    "\n",
    "def spectro_batch(batch_size=10):\n",
    "\treturn spectro_batch_generator(batch_size)\n",
    "\n",
    "def speaker(filename):  # vom Dateinamen\n",
    "\t# if not \"_\" in file:\n",
    "\t#   return \"Unknown\"\n",
    "\treturn filename.split(\"_\")[1]\n",
    "\n",
    "def get_speakers(path=pcm_path):\n",
    "\tmaybe_download(Source.DIGIT_SPECTROS)\n",
    "\tmaybe_download(Source.DIGIT_WAVES)\n",
    "\tfiles = os.listdir(path)\n",
    "\tdef nobad(name):\n",
    "\t\treturn \"_\" in name and not \".\" in name.split(\"_\")[1]\n",
    "\tspeakers=list(set(map(speaker,filter(nobad,files))))\n",
    "\tprint(len(speakers),\" speakers: \",speakers)\n",
    "\treturn speakers\n",
    "\n",
    "def load_wav_file(name):\n",
    "\tf = wave.open(name, \"rb\")\n",
    "\t# print(\"loading %s\"%name)\n",
    "\tchunk = []\n",
    "\tdata0 = f.readframes(CHUNK)\n",
    "\twhile data0:  # f.getnframes()\n",
    "\t\t# data=numpy.fromstring(data0, dtype='float32')\n",
    "\t\t# data = numpy.fromstring(data0, dtype='uint16')\n",
    "\t\tdata = numpy.fromstring(data0, dtype='uint8')\n",
    "\t\tdata = (data + 128) / 255.  # 0-1 for Better convergence\n",
    "\t\t# chunks.append(data)\n",
    "\t\tchunk.extend(data)\n",
    "\t\tdata0 = f.readframes(CHUNK)\n",
    "\t# finally trim:\n",
    "\tchunk = chunk[0:CHUNK * 2]  # should be enough for now -> cut\n",
    "\tchunk.extend(numpy.zeros(CHUNK * 2 - len(chunk)))  # fill with padding 0's\n",
    "\t# print(\"%s loaded\"%name)\n",
    "\treturn chunk\n",
    "\n",
    "\n",
    "def spectro_batch_generator(batch_size=10,width=64,source_data=Source.DIGIT_SPECTROS,target=Target.digits):\n",
    "\t# maybe_download(Source.NUMBER_IMAGES , DATA_DIR)\n",
    "\t# maybe_download(Source.SPOKEN_WORDS, DATA_DIR)\n",
    "\tpath=maybe_download(source_data, DATA_DIR)\n",
    "\tpath=path.replace(\"_spectros\",\"\")# HACK! remove!\n",
    "\theight = width\n",
    "\tbatch = []\n",
    "\tlabels = []\n",
    "\tspeakers=get_speakers(path)\n",
    "\tif target==Target.digits: num_classes=10\n",
    "\tif target==Target.first_letter: num_classes=32\n",
    "\tif target==Target.speech_commands: num_classes=len(speech_commands)\n",
    "\tfiles = os.listdir(path)\n",
    "\t# shuffle(files) # todo : split test_fraction batch here!\n",
    "\t# files=files[0:int(len(files)*(1-test_fraction))]\n",
    "\tprint(\"Got %d source data files from %s\"%(len(files),path))\n",
    "\twhile True:\n",
    "\t\t# print(\"shuffling source data files\")\n",
    "\t\tshuffle(files)\n",
    "\t\tfor image_name in files:\n",
    "\t\t\tif not \"_\" in image_name: continue # bad !?!\n",
    "\t\t\timage = skimage.io.imread(path + \"/\" + image_name).astype(numpy.float32)\n",
    "\t\t\t# image.resize(width,height) # lets see ...\n",
    "\t\t\tdata = image / 255.  # 0-1 for Better convergence\n",
    "\t\t\t# data = data.reshape([width * height])  # tensorflow matmul needs flattened matrices wtf\n",
    "\t\t\tbatch.append(list(data))\n",
    "\t\t\t# classe=(ord(image_name[0]) - 48)  # -> 0=0 .. A:65-48 ... 74 for 'z'\n",
    "\t\t\tclasse = (ord(image_name[0]) - 48) % 32# -> 0=0  17 for A, 10 for z ;)\n",
    "\t\t\tlabels.append(dense_to_one_hot(classe,num_classes))\n",
    "\t\t\tif len(batch) >= batch_size:\n",
    "\t\t\t\tyield batch, labels\n",
    "\t\t\t\tbatch = []  # Reset for next batch\n",
    "\t\t\t\tlabels = []\n",
    "\n",
    "def mfcc_batch_generator(batch_size=10, source=Source.DIGIT_WAVES, target=Target.digits):\n",
    "\tmaybe_download(source, DATA_DIR)\n",
    "\tif target == Target.speaker: speakers = get_speakers()\n",
    "\tbatch_features = []\n",
    "\tlabels = []\n",
    "\tfiles = os.listdir(path)\n",
    "\twhile True:\n",
    "\t\tprint(\"loaded batch of %d files\" % len(files))\n",
    "\t\tshuffle(files)\n",
    "\t\tfor file in files:\n",
    "\t\t\tif not file.endswith(\".wav\"): continue\n",
    "\t\t\twave, sr = librosa.load(path+file, mono=True)\n",
    "\t\t\tmfcc = librosa.feature.mfcc(wave, sr)\n",
    "\t\t\tif target==Target.speaker: label=one_hot_from_item(speaker(file), speakers)\n",
    "\t\t\telif target==Target.digits:  label=dense_to_one_hot(int(file[0]),10)\n",
    "\t\t\telif target==Target.first_letter:  label=dense_to_one_hot((ord(file[0]) - 48) % 32,32)\n",
    "\t\t\telif target == Target.hotword: label = one_hot_word(file, pad_to=max_word_length)  #\n",
    "\t\t\telif target == Target.word: label=string_to_int_word(file, pad_to=max_word_length)\n",
    "\t\t\t\t# label = file  # sparse_labels(file, pad_to=20)  # max_output_length\n",
    "\t\t\telse: raise Exception(\"todo : labels for Target!\")\n",
    "\t\t\tlabels.append(label)\n",
    "\t\t\t# print(np.array(mfcc).shape)\n",
    "\t\t\tmfcc=np.pad(mfcc,((0,0),(0,80-len(mfcc[0]))), mode='constant', constant_values=0)\n",
    "\t\t\tbatch_features.append(np.array(mfcc))\n",
    "\t\t\tif len(batch_features) >= batch_size:\n",
    "\t\t\t\t# if target == Target.word:  labels = sparse_labels(labels)\n",
    "\t\t\t\t# labels=np.array(labels)\n",
    "\t\t\t\t# print(np.array(batch_features).shape)\n",
    "\t\t\t\t# yield np.array(batch_features), labels\n",
    "\t\t\t\t# print(np.array(labels).shape) # why (64,) instead of (64, 15, 32)? OK IFF dim_1==const (20)\n",
    "\t\t\t\tyield batch_features, labels  # basic_rnn_seq2seq inputs must be a sequence\n",
    "\t\t\t\tbatch_features = []  # Reset for next batch\n",
    "\t\t\t\tlabels = []\n",
    "\n",
    "\n",
    "# If you set dynamic_pad=True when calling tf.train.batch the returned batch will be automatically padded with 0s. Handy! A lower-level option is to use tf.PaddingFIFOQueue.\n",
    "# only apply to a subset of all images at one time\n",
    "def wave_batch_generator(batch_size=10,source=Source.DIGIT_WAVES,target=Target.digits): #speaker\n",
    "\tmaybe_download(source, DATA_DIR)\n",
    "\tif target == Target.speaker: speakers=get_speakers()\n",
    "\tbatch_waves = []\n",
    "\tlabels = []\n",
    "\t# input_width=CHUNK*6 # wow, big!!\n",
    "\tfiles = os.listdir(path)\n",
    "\twhile True:\n",
    "\t\tshuffle(files)\n",
    "\t\tprint(\"loaded batch of %d files\" % len(files))\n",
    "\t\tfor wav in files:\n",
    "\t\t\tif not wav.endswith(\".wav\"):continue\n",
    "\t\t\tif target==Target.digits: labels.append(dense_to_one_hot(int(wav[0])))\n",
    "\t\t\telif target==Target.speaker: labels.append(one_hot_from_item(speaker(wav), speakers))\n",
    "\t\t\telif target==Target.first_letter:  label=dense_to_one_hot((ord(wav[0]) - 48) % 32,32)\n",
    "\t\t\telse: raise Exception(\"todo : Target.word label!\")\n",
    "\t\t\tchunk = load_wav_file(path+wav)\n",
    "\t\t\tbatch_waves.append(chunk)\n",
    "\t\t\t# batch_waves.append(chunks[input_width])\n",
    "\t\t\tif len(batch_waves) >= batch_size:\n",
    "\t\t\t\tyield batch_waves, labels\n",
    "\t\t\t\tbatch_waves = []  # Reset for next batch\n",
    "\t\t\t\tlabels = []\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "\tdef __init__(self, images, labels, fake_data=False, one_hot=False, load=False):\n",
    "\t\t\"\"\"Construct a DataSet. one_hot arg is used only if fake_data is true.\"\"\"\n",
    "\t\tif fake_data:\n",
    "\t\t\tself._num_examples = 10000\n",
    "\t\t\tself.one_hot = one_hot\n",
    "\t\telse:\n",
    "\t\t\tnum = len(images)\n",
    "\t\t\tassert num == len(labels), ('images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n",
    "\t\t\tprint(\"len(images) %d\" % num)\n",
    "\t\t\tself._num_examples = num\n",
    "\t\tself.cache={}\n",
    "\t\tself._image_names = numpy.array(images)\n",
    "\t\tself._labels = labels\n",
    "\t\tself._epochs_completed = 0\n",
    "\t\tself._index_in_epoch = 0\n",
    "\t\tself._images=[]\n",
    "\t\tif load: # Otherwise loaded on demand\n",
    "\t\t\tself._images=self.load(self._image_names)\n",
    "\n",
    "\t@property\n",
    "\tdef images(self):\n",
    "\t\treturn self._images\n",
    "\n",
    "\t@property\n",
    "\tdef image_names(self):\n",
    "\t\treturn self._image_names\n",
    "\n",
    "\t@property\n",
    "\tdef labels(self):\n",
    "\t\treturn self._labels\n",
    "\n",
    "\t@property\n",
    "\tdef num_examples(self):\n",
    "\t\treturn self._num_examples\n",
    "\n",
    "\t@property\n",
    "\tdef epochs_completed(self):\n",
    "\t\treturn self._epochs_completed\n",
    "\n",
    "\t# only apply to a subset of all images at one time\n",
    "\tdef load(self,image_names):\n",
    "\t\tprint(\"loading %d images\"%len(image_names))\n",
    "\t\treturn list(map(self.load_image,image_names)) # python3 map object WTF\n",
    "\n",
    "\tdef load_image(self,image_name):\n",
    "\t\tif image_name in self.cache:\n",
    "\t\t\t\treturn self.cache[image_name]\n",
    "\t\telse:\n",
    "\t\t\timage = skimage.io.imread(DATA_DIR+ image_name).astype(numpy.float32)\n",
    "\t\t\t# images = numpy.multiply(images, 1.0 / 255.0)\n",
    "\t\t\tself.cache[image_name]=image\n",
    "\t\t\treturn image\n",
    "\n",
    "\n",
    "\tdef next_batch(self, batch_size, fake_data=False):\n",
    "\t\t\"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "\t\tif fake_data:\n",
    "\t\t\tfake_image = [1] * width * height\n",
    "\t\t\tif self.one_hot:\n",
    "\t\t\t\tfake_label = [1] + [0] * 9\n",
    "\t\t\telse:\n",
    "\t\t\t\tfake_label = 0\n",
    "\t\t\treturn [fake_image for _ in xrange(batch_size)], [\n",
    "\t\t\t\t\tfake_label for _ in xrange(batch_size)]\n",
    "\t\tstart = self._index_in_epoch\n",
    "\t\tself._index_in_epoch += batch_size\n",
    "\t\tif self._index_in_epoch > self._num_examples:\n",
    "\t\t\t# Finished epoch\n",
    "\t\t\tself._epochs_completed += 1\n",
    "\t\t\t# Shuffle the data\n",
    "\t\t\tperm = numpy.arange(self._num_examples)\n",
    "\t\t\tnumpy.random.shuffle(perm)\n",
    "\t\t\t# self._images = self._images[perm]\n",
    "\t\t\tself._image_names = self._image_names[perm]\n",
    "\t\t\tself._labels = self._labels[perm]\n",
    "\t\t\t# Start next epoch\n",
    "\t\t\tstart = 0\n",
    "\t\t\tself._index_in_epoch = batch_size\n",
    "\t\t\tassert batch_size <= self._num_examples\n",
    "\t\tend = self._index_in_epoch\n",
    "\t\treturn self.load(self._image_names[start:end]), self._labels[start:end]\n",
    "\n",
    "\n",
    "# multi-label\n",
    "def dense_to_some_hot(labels_dense, num_classes=140):\n",
    "\t\"\"\"Convert class labels from int vectors to many-hot vectors!\"\"\"\n",
    "\traise \"TODO dense_to_some_hot\"\n",
    "\n",
    "\n",
    "def one_hot_to_item(hot, items):\n",
    "\ti=np.argmax(hot)\n",
    "\titem=items[i]\n",
    "\treturn item\n",
    "\n",
    "def one_hot_from_item(item, items):\n",
    "\t# items=set(items) # assure uniqueness\n",
    "\tx=[0]*len(items)# numpy.zeros(len(items))\n",
    "\ti=items.index(item)\n",
    "\tx[i]=1\n",
    "\treturn x\n",
    "\n",
    "\n",
    "def one_hot_word(word,pad_to=max_word_length):\n",
    "\tvec=[]\n",
    "\tfor c in word:#.upper():\n",
    "\t\tx = [0] * num_characters\n",
    "\t\tx[(ord(c) - offset)%num_characters]=1\n",
    "\t\tvec.append(x)\n",
    "\tif pad_to:vec=pad(vec, pad_to, one_hot=True)\n",
    "\treturn vec\n",
    "\n",
    "def many_hot_to_word(word):\n",
    "\ts=\"\"\n",
    "\tfor c in word:\n",
    "\t\tx=np.argmax(c)\n",
    "\t\ts+=chr(x+offset)\n",
    "\t\t# s += chr(x + 48) # numbers\n",
    "\treturn s\n",
    "\n",
    "\n",
    "def dense_to_one_hot(batch, batch_size, num_labels):\n",
    "\tsparse_labels = tf.reshape(batch, [batch_size, 1])\n",
    "\tindices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n",
    "\tconcatenated = tf.concat(axis=1, values=[indices, sparse_labels])\n",
    "\tconcat = tf.concat(axis=0, values=[[batch_size], [num_labels]])\n",
    "\toutput_shape = tf.reshape(concat, [2])\n",
    "\tsparse_to_dense = tf.sparse_to_dense(concatenated, output_shape, 1.0, 0.0)\n",
    "\treturn tf.reshape(sparse_to_dense, [batch_size, num_labels])\n",
    "\n",
    "\n",
    "def dense_to_one_hot(batch, batch_size, num_labels):\n",
    "\tsparse_labels = tf.reshape(batch, [batch_size, 1])\n",
    "\tindices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n",
    "\tconcatenated = tf.concat(axis=1, values=[indices, sparse_labels])\n",
    "\tconcat = tf.concat(axis=0, values=[[batch_size], [num_labels]])\n",
    "\toutput_shape = tf.reshape(concat, [2])\n",
    "\tsparse_to_dense = tf.sparse_to_dense(concatenated, output_shape, 1.0, 0.0)\n",
    "\treturn tf.reshape(sparse_to_dense, [batch_size, num_labels])\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "\t\"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "\treturn numpy.eye(num_classes)[labels_dense]\n",
    "\n",
    "def extract_labels(names_file,train, one_hot):\n",
    "\tlabels=[]\n",
    "\tfor line in open(names_file).readlines():\n",
    "\t\timage_file,image_label = line.split(\"\\t\")\n",
    "\t\tlabels.append(image_label)\n",
    "\tif one_hot:\n",
    "\t\t\treturn dense_to_one_hot(labels)\n",
    "\treturn labels\n",
    "\n",
    "def extract_images(names_file,train):\n",
    "\timage_files=[]\n",
    "\tfor line in open(names_file).readlines():\n",
    "\t\timage_file,image_label = line.split(\"\\t\")\n",
    "\t\timage_files.append(image_file)\n",
    "\treturn image_files\n",
    "\n",
    "\n",
    "def read_data_sets(train_dir,source_data=Source.NUMBER_IMAGES, fake_data=False, one_hot=True):\n",
    "\tclass DataSets(object):\n",
    "\t\tpass\n",
    "\tdata_sets = DataSets()\n",
    "\tif fake_data:\n",
    "\t\tdata_sets.train = DataSet([], [], fake_data=True, one_hot=one_hot)\n",
    "\t\tdata_sets.validation = DataSet([], [], fake_data=True, one_hot=one_hot)\n",
    "\t\tdata_sets.test = DataSet([], [], fake_data=True, one_hot=one_hot)\n",
    "\t\treturn data_sets\n",
    "\tVALIDATION_SIZE = 2000\n",
    "\tlocal_file = maybe_download(source_data, train_dir)\n",
    "\ttrain_images = extract_images(TRAIN_INDEX,train=True)\n",
    "\ttrain_labels = extract_labels(TRAIN_INDEX,train=True, one_hot=one_hot)\n",
    "\ttest_images = extract_images(TEST_INDEX,train=False)\n",
    "\ttest_labels = extract_labels(TEST_INDEX,train=False, one_hot=one_hot)\n",
    "\t# train_images = train_images[:VALIDATION_SIZE]\n",
    "\t# train_labels = train_labels[:VALIDATION_SIZE:]\n",
    "\t# test_images = test_images[VALIDATION_SIZE:]\n",
    "\t# test_labels = test_labels[VALIDATION_SIZE:]\n",
    "\tdata_sets.train = DataSet(train_images, train_labels , load=False)\n",
    "\tdata_sets.test = DataSet(test_images, test_labels, load=True)\n",
    "\t# data_sets.validation = DataSet(validation_images, validation_labels, load=True)\n",
    "\treturn data_sets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tprint(\"downloading speech datasets\")\n",
    "\tmaybe_download( Source.DIGIT_SPECTROS)\n",
    "\tmaybe_download( Source.DIGIT_WAVES)\n",
    "\tmaybe_download( Source.NUMBER_IMAGES)\n",
    "\tmaybe_download( Source.NUMBER_WAVES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
